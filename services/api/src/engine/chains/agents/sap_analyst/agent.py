"""
SAP Analyst Agent - AnÃ¡lisis de datos y reportes SAP.

Subagente especializado en:
- ConexiÃ³n a sistemas SAP (S/4HANA, ECC, BI/BW)
- ExtracciÃ³n de datos vÃ­a RFC, OData, queries
- AnÃ¡lisis estadÃ­stico y generaciÃ³n de insights
- CreaciÃ³n de reportes y dashboards

Sistema de Skills: carga conocimiento especializado segÃºn el tipo de anÃ¡lisis.
"""

import json
import time
from pathlib import Path
from typing import Optional, List, Dict, Any
from datetime import datetime

import structlog

from ..base import BaseSubAgent, SubAgentResult, Skill

logger = structlog.get_logger()


def _read_system_prompt() -> str:
    """Lee el prompt de sistema desde fichero."""
    path = Path(__file__).parent / "prompts" / "system_prompt.txt"
    try:
        return path.read_text(encoding="utf-8")
    except (FileNotFoundError, OSError):
        return """Eres un Analista de Datos SAP BIW (Business Intelligence Warehouse) experto. Tu misiÃ³n es extraer, analizar y reportar 
datos desde sistemas SAP BW/BI segÃºn las necesidades del usuario.

Capacidades:
- Extraer datos desde InfoCubes (cubos OLAP) multidimensionales
- Consultar DSOs (DataStore Objects) para datos maestros y transaccionales
- Ejecutar queries BEx con filtros, variables y selecciones
- Obtener jerarquÃ­as, atributos y datos maestros
- Trabajar con Key Figures (ratios) y Characteristics (caracterÃ­sticas)
- AnÃ¡lisis estadÃ­stico: tendencias, correlaciones, anomalÃ­as
- Generar reportes ejecutivos con insights accionables
- Exportar datos en mÃºltiples formatos

Herramientas BIW disponibles:
- biw_get_cube_data: Extraer datos de InfoCubes con navegaciÃ³n multidimensional
- biw_get_dso_data: Extraer datos de DataStore Objects
- biw_get_bex_query: Ejecutar queries BEx con parÃ¡metros
- biw_get_master_data: Obtener datos maestros con atributos
- biw_get_hierarchy: Obtener jerarquÃ­as (geogrÃ¡ficas, organizativas, de producto)
- biw_get_texts: Obtener textos descriptivos
- biw_get_ratios: Obtener ratios calculados (KPIs)

Proceso de trabajo:
1. Entender quÃ© datos necesita el usuario (InfoCube, DSO, query BEx)
2. Identificar caracterÃ­sticas (ejes de anÃ¡lisis) y ratios (medidas)
3. Aplicar filtros y variables segÃºn los requerimientos
4. Ejecutar la extracciÃ³n usando las herramientas BIW
5. Analizar los datos segÃºn la peticiÃ³n (anÃ¡lisis multidimensional, tendencias, comparativas)
6. Generar un reporte claro con hallazgos, insights y recomendaciones

Notas importantes:
- Los InfoCubes son multidimensionales: usa caracterÃ­sticas para filtrar y navegar
- Las jerarquÃ­as permiten drill-down (ej: AÃ±o â†’ Trimestre â†’ Mes)
- Los ratios pueden ser simples o calculados (formulas complejas)
- Siempre valida la integridad de los datos y reporta anomalÃ­as

Siempre valida la integridad de los datos y reporta cualquier anomalÃ­a encontrada."""


# Skills disponibles para el SAP BIW Analyst
SAP_ANALYST_SKILLS = [
    Skill(
        id="biw_data_extraction",
        name="ExtracciÃ³n de Datos BIW",
        description="ExtracciÃ³n de datos desde SAP BW/BI: InfoCubes, DSOs, queries BEx, InfoProviders"
    ),
    Skill(
        id="biw_reporting",
        name="Reporting y Analytics BIW",
        description="CreaciÃ³n de reportes BIW: BEx Analyzer, Web Intelligence, dashboards y visualizaciones"
    ),
    Skill(
        id="biw_cube_analysis",
        name="AnÃ¡lisis de InfoCubes",
        description="AnÃ¡lisis multidimensional: navegaciÃ³n por caracterÃ­sticas, ratios clave, agregaciones"
    ),
    Skill(
        id="biw_master_data",
        name="Datos Maestros BIW",
        description="GestiÃ³n de atributos, jerarquÃ­as, textos y datos maestros en BW"
    ),
    Skill(
        id="sap_query_advanced",
        name="Queries SAP Avanzadas",
        description="TÃ©cnicas avanzadas de extracciÃ³n: joins, variants, selecciones dinÃ¡micas, optimizaciÃ³n"
    ),
    Skill(
        id="statistical_methods",
        name="MÃ©todos EstadÃ­sticos",
        description="AnÃ¡lisis estadÃ­stico: regresiÃ³n, correlaciÃ³n, forecasting, anÃ¡lisis de varianza"
    )
]


class SAPAnalystAgent(BaseSubAgent):
    """
    Subagente especializado en anÃ¡lisis de datos SAP BIW (Business Intelligence Warehouse).
    
    Extrae datos desde SAP BW/BI y los analiza segÃºn requerimientos,
    generando reportes con insights y recomendaciones.
    """
    
    id = "sap_analyst"
    name = "SAP BIW Analyst"
    description = "Analista de datos SAP BIW: extracciÃ³n BW, cubos OLAP, reportes y anÃ¡lisis multidimensional"
    version = "1.0.0"
    domain_tools = [
        "biw_get_cube_data",           # Extraer datos de InfoCubes
        "biw_get_dso_data",            # Extraer datos de DSOs
        "biw_get_bex_query",           # Ejecutar queries BEx
        "biw_get_master_data",         # Obtener datos maestros
        "biw_get_hierarchy",           # Obtener jerarquÃ­as
        "biw_get_texts",               # Obtener textos
        "biw_get_ratios",              # Obtener ratios/calculated key figures
        "filesystem",                  # Para leer/escribir archivos
        "execute_code"                 # Para anÃ¡lisis estadÃ­stico
    ]
    available_skills = SAP_ANALYST_SKILLS
    
    role = "Analista de Datos SAP BIW Senior"
    expertise = """Experto en extracciÃ³n y anÃ¡lisis de datos desde SAP BW/BI (Business Intelligence Warehouse).
    
Especializado en:
- InfoCubes (cubos OLAP) y navegaciÃ³n multidimensional
- DSOs (DataStore Objects) para datos maestros y transaccionales
- Queries BEx y reporting estructurado
- JerarquÃ­as, atributos y datos maestros
- Key Figures (ratios) y Characteristics (caracterÃ­sticas)
- Filtros, variables y selecciones dinÃ¡micas

TÃ©cnicas de anÃ¡lisis:
- AnÃ¡lisis multidimensional (slicing, dicing, drill-down)
- Comparativas y variaciones (YoY, MoM)
- Forecasting y tendencias temporales
- ABC analysis y Pareto
- Correlaciones entre KPIs

Genera reportes ejecutivos, dashboards y visualizaciones a partir de datos BW."""
    
    task_requirements = """Para analizar datos SAP BIW necesito:

**Obligatorio:**
- QuÃ© datos necesitas (InfoCube, DSO, query BEx especÃ­fico)
- PerÃ­odo de anÃ¡lisis (fechas desde/hasta o perÃ­odo fiscal)
- CaracterÃ­sticas clave (ej: sociedad, centro, producto, cliente)

**Opcional pero recomendado:**
- QuÃ© tipo de anÃ¡lisis requieres (cubo, tendencias, comparativas, drill-down)
- JerarquÃ­as a utilizar (ej: geografÃ­a, organizaciÃ³n, producto)
- Ratios/KPIs especÃ­ficos a calcular
- Filtros y variables (ej: solo activos, solo 2024, etc.)
- Formato de salida deseado (Excel, CSV, PDF, JSON, tabla)

**Ejemplos:**
- "AnÃ¡lisis de ventas del Ãºltimo trimestre por regiÃ³n y producto desde el cubo ZC_SALES"
- "Drill-down de costes por centro de coste y mes fiscal desde ZC_COSTS"
- "Comparativa YoY de margen por cliente desde la query BEx ZQ_MARGIN"
- "Datos maestros de materiales con atributos desde el DSO ZDSO_MATERIAL"
"""
    
    def __init__(self):
        super().__init__()
        self._connection_cache: Dict[str, Any] = {}
        logger.info(f"ğŸ“Š SAPAnalystAgent initialized with {len(self.available_skills)} skills")
    
    async def execute(
        self,
        task: str,
        context: Optional[str] = None,
        llm_url: Optional[str] = None,
        model: Optional[str] = None,
        provider_type: str = "ollama",
        api_key: Optional[str] = None
    ) -> SubAgentResult:
        """
        Ejecuta anÃ¡lisis de datos SAP segÃºn la peticiÃ³n.
        
        Args:
            task: DescripciÃ³n de la tarea de anÃ¡lisis
            context: Contexto adicional (datos de conexiÃ³n, configuraciones)
            llm_url: URL del LLM para procesamiento
            model: Modelo LLM a usar
            provider_type: Tipo de proveedor LLM
            api_key: API key para LLM
            
        Returns:
            SubAgentResult con el reporte de anÃ¡lisis
        """
        start_time = time.time()
        logger.info("ğŸ“Š SAPAnalystAgent executing", task=task[:80])
        
        try:
            # Parsear la peticiÃ³n
            task_data = self._parse_task(task)
            
            # Validar requisitos mÃ­nimos
            validation = self._validate_requirements(task_data)
            if not validation["valid"]:
                return SubAgentResult(
                    success=False,
                    response=f"âŒ **Faltan datos obligatorios:**\n\n{validation['message']}\n\nPor favor, proporciona la informaciÃ³n necesaria e intenta de nuevo.",
                    agent_id=self.id,
                    agent_name=self.name,
                    error="Missing required parameters",
                    execution_time_ms=int((time.time() - start_time) * 1000)
                )
            
            # Determinar skills relevantes y cargarlos
            relevant_skills = self._determine_relevant_skills(task_data)
            loaded_skills_content = []
            for skill_id in relevant_skills:
                skill_result = self.load_skill(skill_id)
                if skill_result["success"]:
                    loaded_skills_content.append(f"\n### {skill_id}\n{skill_result['content']}")
            
            # Fase 1: ExtracciÃ³n de datos
            extraction_result = await self._extract_data(
                task_data, 
                context,
                loaded_skills_content
            )
            
            if not extraction_result["success"]:
                return SubAgentResult(
                    success=False,
                    response=f"âŒ **Error en extracciÃ³n de datos:**\n\n{extraction_result.get('error', 'Error desconocido')}",
                    agent_id=self.id,
                    agent_name=self.name,
                    error=extraction_result.get("error"),
                    execution_time_ms=int((time.time() - start_time) * 1000)
                )
            
            # Fase 2: AnÃ¡lisis de datos
            analysis_result = await self._analyze_data(
                task_data,
                extraction_result["data"],
                loaded_skills_content,
                llm_url,
                model,
                provider_type,
                api_key
            )
            
            # Fase 3: Generar reporte final
            report = self._generate_report(
                task_data,
                extraction_result,
                analysis_result
            )
            
            execution_time = int((time.time() - start_time) * 1000)
            
            return SubAgentResult(
                success=True,
                response=report,
                agent_id=self.id,
                agent_name=self.name,
                tools_used=["execute_code", "filesystem"],
                data={
                    "extraction": extraction_result,
                    "analysis": analysis_result,
                    "skills_used": relevant_skills,
                    "execution_time_ms": execution_time
                },
                execution_time_ms=execution_time
            )
            
        except Exception as e:
            logger.error(f"SAPAnalystAgent error: {e}", exc_info=True)
            return SubAgentResult(
                success=False,
                response=f"âŒ **Error en anÃ¡lisis SAP:** {str(e)}",
                agent_id=self.id,
                agent_name=self.name,
                error=str(e),
                execution_time_ms=int((time.time() - start_time) * 1000)
            )
    
    def _parse_task(self, task: str) -> Dict[str, Any]:
        """Parsea la tarea en componentes estructurados."""
        try:
            # Intentar parsear como JSON
            data = json.loads(task)
            return {
                "description": data.get("task", data.get("description", task)),
                "module": data.get("module", ""),
                "tables": data.get("tables", []),
                "fields": data.get("fields", []),
                "period": data.get("period", {}),
                "filters": data.get("filters", {}),
                "analysis_type": data.get("analysis_type", "descriptive"),
                "output_format": data.get("output_format", "report"),
                "comparison": data.get("comparison", False),
                "group_by": data.get("group_by", []),
                "metrics": data.get("metrics", []),
                "connection": data.get("connection", {}),
                "raw": task
            }
        except (json.JSONDecodeError, TypeError):
            # Parseo manual desde texto libre
            return {
                "description": task,
                "module": self._extract_module(task),
                "tables": [],
                "fields": [],
                "period": self._extract_period(task),
                "filters": {},
                "analysis_type": self._detect_analysis_type(task),
                "output_format": "report",
                "comparison": self._detect_comparison(task),
                "group_by": [],
                "metrics": [],
                "connection": {},
                "raw": task
            }
    
    def _extract_module(self, text: str) -> str:
        """Extrae el mÃ³dulo SAP mencionado."""
        text_lower = text.lower()
        modules = {
            "fi": ["financiero", "contable", "balance", "cuenta", "fi/"],
            "co": ["controlling", "coste", "centro de coste", "profit center", "co/"],
            "sd": ["ventas", "cliente", "pedido", "factura", "sd/", "billing"],
            "mm": ["material", "inventario", "stock", "compra", "proveedor", "mm/"],
            "pp": ["producciÃ³n", "orden", "fabricaciÃ³n", "pp/", "bom"],
            "hr": ["personal", "empleado", "nÃ³mina", "hr/", "hcm", "recursos humanos"]
        }
        for mod, keywords in modules.items():
            if any(kw in text_lower for kw in keywords):
                return mod.upper()
        return ""
    
    def _extract_period(self, text: str) -> Dict[str, str]:
        """Extrae el perÃ­odo de anÃ¡lisis."""
        import re
        period = {}
        
        # Buscar fechas
        date_patterns = [
            r'(\d{4}-\d{2}-\d{2})',
            r'((?:ene|feb|mar|abr|may|jun|jul|ago|sep|oct|nov|dic)[a-z]*)',
            r'(trimestre|quarter|q)[ ]?([1-4])',
            r'(aÃ±o|year)[ ]?(20)?(19)?(20)?(21)?(22)?(23)?(24)?(25)?(26)?(27)?(28)?(29)?',
        ]
        
        # Patrones de perÃ­odo comunes
        if "Ãºltimo mes" in text.lower() or "last month" in text.lower():
            period["type"] = "last_month"
        elif "Ãºltimo trimestre" in text.lower() or "last quarter" in text.lower():
            period["type"] = "last_quarter"
        elif "Ãºltimo aÃ±o" in text.lower() or "last year" in text.lower():
            period["type"] = "last_year"
        elif "aÃ±o anterior" in text.lower() or "previous year" in text.lower():
            period["type"] = "previous_year"
        elif "ytd" in text.lower() or "year to date" in text.lower():
            period["type"] = "ytd"
        else:
            period["type"] = "custom"
        
        return period
    
    def _detect_analysis_type(self, text: str) -> str:
        """Detecta el tipo de anÃ¡lisis requerido."""
        text_lower = text.lower()
        if any(kw in text_lower for kw in ["tendencia", "trend", "forecast", "predicciÃ³n", "projection"]):
            return "forecasting"
        elif any(kw in text_lower for kw in ["comparar", "comparativa", "vs", "versus", "comparaciÃ³n", "comparison"]):
            return "comparative"
        elif any(kw in text_lower for kw in ["correlaciÃ³n", "correlation", "regresiÃ³n", "regression"]):
            return "correlation"
        elif any(kw in text_lower for kw in ["anomalÃ­a", "anomaly", "outlier", "detecciÃ³n", "detection"]):
            return "anomaly_detection"
        elif any(kw in text_lower for kw in ["abc", "pareto", "clasificaciÃ³n", "classification"]):
            return "abc_analysis"
        else:
            return "descriptive"
    
    def _detect_comparison(self, text: str) -> bool:
        """Detecta si se requiere comparaciÃ³n."""
        text_lower = text.lower()
        return any(kw in text_lower for kw in ["comparar", "comparativa", "vs", "versus", "aÃ±o anterior", "previous year", "contraste"])
    
    def _validate_requirements(self, task_data: Dict[str, Any]) -> Dict[str, Any]:
        """Valida que se proporcionen los requisitos mÃ­nimos."""
        missing = []
        
        if not task_data.get("description"):
            missing.append("DescripciÃ³n de los datos a analizar")
        
        if not task_data.get("module") and not task_data.get("tables"):
            missing.append("MÃ³dulo SAP o tablas de origen (ej: FI, SD, VBAK, BSEG)")
        
        if not task_data.get("period"):
            missing.append("PerÃ­odo de anÃ¡lisis (fechas o perÃ­odo relativo)")
        
        if missing:
            return {
                "valid": False,
                "message": "\n".join([f"â€¢ {m}" for m in missing])
            }
        
        return {"valid": True, "message": ""}
    
    def _determine_relevant_skills(self, task_data: Dict[str, Any]) -> List[str]:
        """Determina quÃ© skills son relevantes para la tarea."""
        skills = []
        module = task_data.get("module", "").lower()
        analysis_type = task_data.get("analysis_type", "")
        
        # Skills por mÃ³dulo
        if module in ["fi", "co"]:
            skills.append("financial_analysis")
        elif module == "sd":
            skills.append("sales_analysis")
        elif module == "mm":
            skills.append("inventory_analysis")
        elif module == "pp":
            skills.append("production_analysis")
        elif module == "hr":
            skills.append("hr_analysis")
        
        # Skills por tipo de anÃ¡lisis
        if analysis_type in ["forecasting", "correlation", "anomaly_detection", "abc_analysis"]:
            skills.append("statistical_methods")
        
        # Skill de queries avanzadas siempre Ãºtil
        skills.append("sap_query_advanced")
        
        return skills
    
    async def _extract_data(
        self,
        task_data: Dict[str, Any],
        context: Optional[str],
        skills_content: List[str]
    ) -> Dict[str, Any]:
        """Extrae datos desde SAP segÃºn la configuraciÃ³n."""
        
        # Generar cÃ³digo de extracciÃ³n basado en el contexto y skills
        extraction_code = self._generate_extraction_code(task_data, context, skills_content)
        
        logger.info("ğŸ”„ Extracting SAP data", 
                   module=task_data.get("module"), 
                   analysis_type=task_data.get("analysis_type"))
        
        # Ejecutar cÃ³digo de extracciÃ³n
        try:
            from src.tools.core.execution import execute_code
            
            exec_result = await execute_code(
                language="python",
                code=extraction_code,
                timeout=120
            )
            
            if exec_result.get("success"):
                # Parsear resultado
                output = exec_result.get("output", "")
                try:
                    # Intentar extraer JSON del output
                    json_start = output.find('{"data":')
                    if json_start == -1:
                        json_start = output.find('{"status":')
                    
                    if json_start != -1:
                        json_str = output[json_start:]
                        result_data = json.loads(json_str)
                        return {
                            "success": True,
                            "data": result_data.get("data", {}),
                            "metadata": result_data.get("metadata", {}),
                            "rows": result_data.get("metadata", {}).get("rows", 0),
                            "source": task_data.get("module", "SAP")
                        }
                except:
                    pass
                
                return {
                    "success": True,
                    "data": {"raw_output": output},
                    "metadata": {},
                    "rows": 0,
                    "source": task_data.get("module", "SAP")
                }
            else:
                return {
                    "success": False,
                    "error": exec_result.get("error", "ExtracciÃ³n fallida"),
                    "output": exec_result.get("output", "")
                }
                
        except Exception as e:
            logger.error(f"Data extraction error: {e}")
            return {
                "success": False,
                "error": f"Error en extracciÃ³n: {str(e)}"
            }
    
    def _generate_extraction_code(
        self,
        task_data: Dict[str, Any],
        context: Optional[str],
        skills_content: List[str]
    ) -> str:
        """Genera cÃ³digo Python para extraer datos de SAP."""
        
        module = task_data.get("module", "GENERIC")
        tables = task_data.get("tables", [])
        fields = task_data.get("fields", [])
        period = task_data.get("period", {})
        filters = task_data.get("filters", {})
        
        # CÃ³digo base que simula extracciÃ³n SAP
        code = f'''#!/usr/bin/env python3
"""
ExtracciÃ³n de datos SAP - {module}
Generado automÃ¡ticamente por SAPAnalystAgent
"""

import json
import pandas as pd
from datetime import datetime, timedelta
import numpy as np

# SimulaciÃ³n de conexiÃ³n SAP (en producciÃ³n usar pyrfc, pyodata, etc.)
def extract_sap_data():
    """Extrae datos desde SAP."""
    
    # ConfiguraciÃ³n de extracciÃ³n
    module = "{module}"
    tables = {tables if tables else ["VBAK", "VBAP", "KNA1" if module == "SD" else "BSEG", "BKPF" if module in ["FI", "CO"] else "MARA", "MARD"]}
    fields = {fields if fields else []}
    
    # PerÃ­odo
    period_config = {dict(period)}
    
    # Generar datos simulados para demostraciÃ³n
    # En producciÃ³n, aquÃ­ irÃ­a la llamada real a SAP
    print(f"ğŸ”— Conectando a SAP ({module})...")
    print(f"ğŸ“Š Extrayendo de tablas: {{', '.join(tables)}}")
    
    # SimulaciÃ³n de datos extraÃ­dos
    np.random.seed(42)
    n_rows = 1000
    
    data = {{}}
    
    if module == "SD":
        # Datos de ventas
        data["sales"] = {{
            "document_number": [f"900000{{i:05d}}" for i in range(n_rows)],
            "customer": np.random.choice(["C001", "C002", "C003", "C004", "C005"], n_rows),
            "material": np.random.choice(["M001", "M002", "M003", "M004"], n_rows),
            "quantity": np.random.randint(1, 100, n_rows),
            "amount": np.round(np.random.uniform(100, 10000, n_rows), 2),
            "order_date": pd.date_range(end=datetime.now(), periods=n_rows, freq='H').tolist(),
            "region": np.random.choice(["Norte", "Sur", "Este", "Oeste"], n_rows)
        }}
    elif module in ["FI", "CO"]:
        # Datos financieros
        data["financial"] = {{
            "document": [f"100000{{i:05d}}" for i in range(n_rows)],
            "account": np.random.choice(["1000", "2000", "3000", "4000"], n_rows),
            "amount": np.round(np.random.uniform(-5000, 5000, n_rows), 2),
            "posting_date": pd.date_range(end=datetime.now(), periods=n_rows, freq='H').tolist(),
            "company_code": np.random.choice(["1000", "2000"], n_rows),
            "cost_center": np.random.choice(["CC01", "CC02", "CC03"], n_rows)
        }}
    elif module == "MM":
        # Datos de inventario
        data["inventory"] = {{
            "material": [f"MAT{{i:05d}}" for i in range(500)],
            "plant": np.random.choice(["1000", "2000", "3000"], 500),
            "stock_quantity": np.random.randint(0, 10000, 500),
            "stock_value": np.round(np.random.uniform(0, 100000, 500), 2),
            "movement_type": np.random.choice(["101", "102", "201", "202", "261", "262"], 500),
            "last_movement": pd.date_range(end=datetime.now(), periods=500, freq='2H').tolist()
        }}
    else:
        # Datos genÃ©ricos
        data["generic"] = {{
            "id": range(n_rows),
            "value": np.random.uniform(0, 100, n_rows),
            "category": np.random.choice(["A", "B", "C"], n_rows),
            "timestamp": pd.date_range(end=datetime.now(), periods=n_rows, freq='H').tolist()
        }}
    
    # Metadata
    metadata = {{
        "module": module,
        "tables": tables,
        "extracted_at": datetime.now().isoformat(),
        "rows": n_rows,
        "period": period_config,
        "filters_applied": {dict(filters)}
    }}
    
    # Output final
    result = {{
        "status": "success",
        "data": data,
        "metadata": metadata
    }}
    
    print(json.dumps(result))
    return result

if __name__ == "__main__":
    extract_sap_data()
'''
        
        return code
    
    async def _analyze_data(
        self,
        task_data: Dict[str, Any],
        data: Dict[str, Any],
        skills_content: List[str],
        llm_url: Optional[str],
        model: Optional[str],
        provider_type: str,
        api_key: Optional[str]
    ) -> Dict[str, Any]:
        """Analiza los datos extraÃ­dos segÃºn los requerimientos."""
        
        analysis_type = task_data.get("analysis_type", "descriptive")
        
        logger.info("ğŸ“ˆ Analyzing data", analysis_type=analysis_type)
        
        # Generar cÃ³digo de anÃ¡lisis
        analysis_code = self._generate_analysis_code(task_data, skills_content)
        
        try:
            from src.tools.core.execution import execute_code
            
            exec_result = await execute_code(
                language="python",
                code=analysis_code,
                timeout=180
            )
            
            if exec_result.get("success"):
                output = exec_result.get("output", "")
                
                # Intentar extraer resultados
                try:
                    json_start = output.find('{"analysis":')
                    if json_start != -1:
                        json_str = output[json_start:]
                        result_data = json.loads(json_str)
                        return {
                            "success": True,
                            "insights": result_data.get("analysis", {}),
                            "statistics": result_data.get("statistics", {}),
                            "recommendations": result_data.get("recommendations", [])
                        }
                except:
                    pass
                
                return {
                    "success": True,
                    "insights": {{"raw_analysis": output}},
                    "statistics": {{}},
                    "recommendations": []
                }
            else:
                return {
                    "success": False,
                    "error": exec_result.get("error", "AnÃ¡lisis fallido"),
                    "insights": {{}},
                    "recommendations": []
                }
                
        except Exception as e:
            logger.error(f"Data analysis error: {e}")
            return {
                "success": False,
                "error": str(e),
                "insights": {{}},
                "recommendations": []
            }
    
    def _generate_analysis_code(
        self,
        task_data: Dict[str, Any],
        skills_content: List[str]
    ) -> str:
        """Genera cÃ³digo Python para anÃ¡lisis de datos."""
        
        analysis_type = task_data.get("analysis_type", "descriptive")
        module = task_data.get("module", "GENERIC")
        comparison = task_data.get("comparison", False)
        
        code = f'''#!/usr/bin/env python3
"""
AnÃ¡lisis de datos SAP - {analysis_type}
Generado automÃ¡ticamente por SAPAnalystAgent
"""

import json
import pandas as pd
import numpy as np
from datetime import datetime, timedelta

# Cargar datos (en producciÃ³n vendrÃ­an del paso anterior)
# AquÃ­ recreamos datos de ejemplo para el anÃ¡lisis

np.random.seed(42)
n_rows = 1000

# Recrear datos segÃºn el mÃ³dulo
if "{module}" == "SD":
    df = pd.DataFrame({{
        "document_number": [f"900000{{i:05d}}" for i in range(n_rows)],
        "customer": np.random.choice(["C001", "C002", "C003", "C004", "C005"], n_rows),
        "material": np.random.choice(["M001", "M002", "M003", "M004"], n_rows),
        "quantity": np.random.randint(1, 100, n_rows),
        "amount": np.round(np.random.uniform(100, 10000, n_rows), 2),
        "order_date": pd.date_range(end=datetime.now(), periods=n_rows, freq='H'),
        "region": np.random.choice(["Norte", "Sur", "Este", "Oeste"], n_rows)
    }})
elif "{module}" in ["FI", "CO"]:
    df = pd.DataFrame({{
        "document": [f"100000{{i:05d}}" for i in range(n_rows)],
        "account": np.random.choice(["1000", "2000", "3000", "4000"], n_rows),
        "amount": np.round(np.random.uniform(-5000, 5000, n_rows), 2),
        "posting_date": pd.date_range(end=datetime.now(), periods=n_rows, freq='H'),
        "company_code": np.random.choice(["1000", "2000"], n_rows),
        "cost_center": np.random.choice(["CC01", "CC02", "CC03"], n_rows)
    }})
elif "{module}" == "MM":
    df = pd.DataFrame({{
        "material": [f"MAT{{i:05d}}" for i in range(500)],
        "plant": np.random.choice(["1000", "2000", "3000"], 500),
        "stock_quantity": np.random.randint(0, 10000, 500),
        "stock_value": np.round(np.random.uniform(0, 100000, 500), 2),
        "movement_type": np.random.choice(["101", "102", "201", "202", "261", "262"], 500),
        "last_movement": pd.date_range(end=datetime.now(), periods=500, freq='2H')
    }})
else:
    df = pd.DataFrame({{
        "id": range(n_rows),
        "value": np.random.uniform(0, 100, n_rows),
        "category": np.random.choice(["A", "B", "C"], n_rows),
        "timestamp": pd.date_range(end=datetime.now(), periods=n_rows, freq='H')
    }})

# AnÃ¡lisis segÃºn tipo
analysis_results = {{}}
statistics = {{}}
recommendations = []

if "{analysis_type}" == "descriptive":
    # EstadÃ­sticas descriptivas
    statistics["summary"] = df.describe().to_dict()
    statistics["total_records"] = len(df)
    
    if "{module}" == "SD":
        analysis_results["total_sales"] = df["amount"].sum()
        analysis_results["avg_order_value"] = df["amount"].mean()
        analysis_results["total_orders"] = len(df)
        analysis_results["top_customers"] = df.groupby("customer")["amount"].sum().sort_values(ascending=False).head(5).to_dict()
        analysis_results["sales_by_region"] = df.groupby("region")["amount"].sum().to_dict()
        analysis_results["top_products"] = df.groupby("material")["amount"].sum().sort_values(ascending=False).head(5).to_dict()
        
        recommendations.append("Los 5 principales clientes generan " + 
            f"{{df.groupby('customer')['amount'].sum().sort_values(ascending=False).head(5).sum() / df['amount'].sum() * 100:.1f}}% del volumen total")
        
    elif "{module}" in ["FI", "CO"]:
        analysis_results["total_debits"] = df[df["amount"] > 0]["amount"].sum()
        analysis_results["total_credits"] = abs(df[df["amount"] < 0]["amount"].sum())
        analysis_results["net_balance"] = df["amount"].sum()
        analysis_results["by_account"] = df.groupby("account")["amount"].sum().to_dict()
        analysis_results["by_cost_center"] = df.groupby("cost_center")["amount"].sum().to_dict()
        
        if analysis_results["net_balance"] > 0:
            recommendations.append(f"Saldo neto positivo de {{analysis_results['net_balance']:,.2f}}. Revisar conciliaciÃ³n.")
        
    elif "{module}" == "MM":
        analysis_results["total_stock_value"] = df["stock_value"].sum()
        analysis_results["avg_stock_quantity"] = df["stock_quantity"].mean()
        analysis_results["high_value_materials"] = df.nlargest(10, "stock_value")[["material", "stock_value"]].to_dict("records")
        analysis_results["zero_stock"] = len(df[df["stock_quantity"] == 0])
        
        if analysis_results["zero_stock"] > 0:
            recommendations.append(f"{{analysis_results['zero_stock']}} materiales sin stock. Revisar planificaciÃ³n.")

elif "{analysis_type}" == "forecasting":
    # AnÃ¡lisis de tendencias simple
    if "order_date" in df.columns or "posting_date" in df.columns:
        date_col = "order_date" if "order_date" in df.columns else "posting_date"
        df["period"] = pd.to_datetime(df[date_col]).dt.to_period('M')
        
        if "amount" in df.columns:
            trend = df.groupby("period")["amount"].sum()
            analysis_results["monthly_trend"] = trend.to_dict()
            analysis_results["trend_direction"] = "creciente" if trend.iloc[-1] > trend.iloc[0] else "decreciente"
            analysis_results["trend_change_pct"] = ((trend.iloc[-1] - trend.iloc[0]) / abs(trend.iloc[0]) * 100) if trend.iloc[0] != 0 else 0
            
            recommendations.append(f"Tendencia {{analysis_results['trend_direction']}} del {{abs(analysis_results['trend_change_pct']):.1f}}% en el perÃ­odo")

elif "{analysis_type}" == "abc_analysis":
    # AnÃ¡lisis ABC/Pareto
    if "amount" in df.columns:
        if "material" in df.columns:
            grouped = df.groupby("material")["amount"].sum().sort_values(ascending=False)
        elif "customer" in df.columns:
            grouped = df.groupby("customer")["amount"].sum().sort_values(ascending=False)
        else:
            grouped = df.groupby(df.index // (len(df) // 10))["amount"].sum().sort_values(ascending=False)
        
        total = grouped.sum()
        cumulative = grouped.cumsum()
        
        # ClasificaciÃ³n ABC
        abc = pd.DataFrame({{
            "value": grouped,
            "percentage": grouped / total * 100,
            "cumulative_pct": cumulative / total * 100
        }})
        
        abc["class"] = pd.cut(abc["cumulative_pct"], 
                             bins=[0, 80, 95, 100], 
                             labels=["A", "B", "C"])
        
        analysis_results["abc_distribution"] = abc.groupby("class").agg({{
            "value": "sum",
            "percentage": "sum"
        }}).to_dict()
        
        analysis_results["class_a_items"] = len(abc[abc["class"] == "A"])
        analysis_results["class_a_contribution"] = abc[abc["class"] == "A"]["percentage"].sum()
        
        recommendations.append(f"Clase A: {{analysis_results['class_a_items']}} items ({{analysis_results['class_a_contribution']:.1f}}% del valor). Priorizar gestiÃ³n.")

# Preparar resultado
result = {{
    "status": "success",
    "analysis": analysis_results,
    "statistics": statistics,
    "recommendations": recommendations,
    "analyzed_at": datetime.now().isoformat()
}}

print(json.dumps(result, indent=2, default=str))
'''
        
        return code
    
    def _generate_report(
        self,
        task_data: Dict[str, Any],
        extraction_result: Dict[str, Any],
        analysis_result: Dict[str, Any]
    ) -> str:
        """Genera el reporte final de anÃ¡lisis."""
        
        module = task_data.get("module", "SAP")
        analysis_type = task_data.get("analysis_type", "descriptive")
        description = task_data.get("description", "AnÃ¡lisis de datos SAP")
        
        parts = [
            f"# ğŸ“Š Reporte de AnÃ¡lisis SAP - {module}",
            f"\n**Tarea:** {description}",
            f"\n**Tipo de anÃ¡lisis:** {analysis_type.title()}",
            f"\n**Fecha:** {datetime.now().strftime('%Y-%m-%d %H:%M')}",
            f"\n---\n"
        ]
        
        # Resumen de extracciÃ³n
        metadata = extraction_result.get("metadata", {})
        parts.append(f"\n## ğŸ“¥ Resumen de ExtracciÃ³n\n")
        parts.append(f"- **Registros extraÃ­dos:** {metadata.get('rows', 'N/A')}")
        parts.append(f"- **MÃ³dulo:** {metadata.get('module', module)}")
        parts.append(f"- **Tablas:** {', '.join(metadata.get('tables', []))}")
        parts.append(f"\n")
        
        # Resultados del anÃ¡lisis
        insights = analysis_result.get("insights", {})
        if insights:
            parts.append(f"\n## ğŸ“ˆ Hallazgos Principales\n")
            
            for key, value in insights.items():
                if isinstance(value, dict):
                    parts.append(f"\n### {key.replace('_', ' ').title()}\n")
                    for k, v in value.items():
                        if isinstance(v, (int, float)):
                            parts.append(f"- **{k}:** {v:,.2f}" if isinstance(v, float) else f"- **{k}:** {v:,}")
                        else:
                            parts.append(f"- **{k}:** {v}")
                elif isinstance(value, list):
                    parts.append(f"\n### {key.replace('_', ' ').title()}\n")
                    for item in value[:5]:  # Limitar a 5 items
                        if isinstance(item, dict):
                            parts.append(f"- {item}")
                        else:
                            parts.append(f"- {item}")
                elif isinstance(value, (int, float)):
                    parts.append(f"\n**{key.replace('_', ' ').title()}:** {value:,.2f}" if isinstance(value, float) else f"\n**{key.replace('_', ' ').title()}:** {value:,}")
                else:
                    parts.append(f"\n**{key.replace('_', ' ').title()}:** {value}")
            
            parts.append(f"\n")
        
        # EstadÃ­sticas
        statistics = analysis_result.get("statistics", {})
        if statistics:
            parts.append(f"\n## ğŸ“Š EstadÃ­sticas\n")
            for key, value in statistics.items():
                parts.append(f"- **{key.replace('_', ' ').title()}:** {value}")
            parts.append(f"\n")
        
        # Recomendaciones
        recommendations = analysis_result.get("recommendations", [])
        if recommendations:
            parts.append(f"\n## ğŸ’¡ Recomendaciones\n")
            for i, rec in enumerate(recommendations, 1):
                parts.append(f"{i}. {rec}")
            parts.append(f"\n")
        
        # Nota sobre datos simulados
        parts.append(f"\n---\n")
        parts.append(f"*âš ï¸ Nota: Este anÃ¡lisis utiliza datos simulados para demostraciÃ³n. En producciÃ³n, se conectarÃ¡ a sistemas SAP reales.*")
        
        return "\n".join(parts)


# Instancia para registro
sap_analyst = SAPAnalystAgent()
